{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f92b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965e3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5fb2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb8f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "Y_train=[]\n",
    "dataTr=[]\n",
    "for filename in glob.glob(os.path.join('D:/UMSA/TESIS/Tesis de grado/data/entrenamiento/Melanoma_escalado','*.jpg')):\n",
    "    dataTr.append([1,cv2.imread(filename)])\n",
    "for filename in glob.glob(os.path.join('D:/UMSA/TESIS/Tesis de grado/data/entrenamiento/Carcinoma_escalado','*.jpg')):\n",
    "    dataTr.append([0,cv2.imread(filename)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d6e8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTe=[]\n",
    "for filename in glob.glob(os.path.join('D:/UMSA/TESIS/Tesis de grado/data/prueba/melanoma_escalado','*.jpg')):\n",
    "    dataTe.append([1,cv2.imread(filename)])\n",
    "for filename in glob.glob(os.path.join('D:/UMSA/TESIS/Tesis de grado/data/prueba/carcinoma_escalado','*.jpg')):\n",
    "    dataTe.append([0,cv2.imread(filename)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "235b98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def particion(datos):\n",
    "    imagenes=[]\n",
    "    etiquetas=[]\n",
    "    for i,j in datos:\n",
    "        imagenes.append(j)\n",
    "        etiquetas.append(i)\n",
    "    imagenes=np.array(imagenes)\n",
    "    etiquetas=np.array(etiquetas)\n",
    "    return (imagenes,etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1079c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en total tenemos: 12279 imagenes dentro de la carpeta train\n",
      "en total tenemos: 5261 imagenes dentro de la carpeta test\n"
     ]
    }
   ],
   "source": [
    "shuffle(dataTr)\n",
    "print(\"en total tenemos: \"+str(len(dataTr))+ \" imagenes dentro de la carpeta train\")\n",
    "shuffle(dataTe)\n",
    "print(\"en total tenemos: \"+str(len(dataTe))+ \" imagenes dentro de la carpeta test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee439097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "para entrenamiento tendremos: 12000 imagenes de la carpeta de train\n",
      "para prueba tendremos: 5000 imagenes de la carpeta de test\n"
     ]
    }
   ],
   "source": [
    "porcion1=dataTr[0:12000]\n",
    "porcion2=dataTe[0:5000]\n",
    "print(\"para entrenamiento tendremos: \"+str(len(porcion1))+ \" imagenes de la carpeta de train\")\n",
    "print(\"para prueba tendremos: \"+str(len(porcion2))+ \" imagenes de la carpeta de test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cccc92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_e,y_e=particion(porcion1)\n",
    "x_p,y_p=particion(porcion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b06adaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacion(x_e,y_e,x_p,y_p,modelo1,epocas):\n",
    "    entre=modelo1.fit(x_e,y_e,batch_size=32,epochs=epocas,validation_data=(x_p,y_p))\n",
    "    a=modelo1.evaluate(x_p,y_p)\n",
    "    return a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa8b8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluacion(modelo1,porcentaje,nombre,v1,v2):\n",
    "    prediccion=0\n",
    "    epocas=1\n",
    "    while(True):\n",
    "        if prediccion < porcentaje:\n",
    "            prediccion=(validacion(x_e,y_e,x_p,y_p,modelo1,1))*100\n",
    "            epocas +=1\n",
    "            v1.append(epocas-1)\n",
    "            v2.append(prediccion)\n",
    "        else:\n",
    "            print(\"==> Para el metodo \"+nombre+\" se utilizo: \"+str(epocas-1)+\" epocas para llegar a mas del \"+str(porcentaje)+\"% de acertividad\")\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3a98edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Desktop\\.conda\\envs\\tesis\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 222, 222, 8)       224       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 220, 220, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 110, 110, 16)      0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 193600)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               24780928  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 24,805,421\n",
      "Trainable params: 24,805,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo=Sequential()\n",
    "modelo.add(Convolution2D(8,(3,3),input_shape=(224,224,3),activation='relu'))\n",
    "modelo.add(Convolution2D(16,(3,3),activation='relu'))\n",
    "modelo.add(MaxPooling2D(pool_size=((2,2))))\n",
    "modelo.add(Flatten())\n",
    "modelo.add(Dense(128,activation='relu'))\n",
    "modelo.add(Dense(100,activation='relu'))\n",
    "modelo.add(Dense(100,activation='relu'))\n",
    "modelo.add(Dense(1,activation='sigmoid'))\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355d8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1115\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 132s 11ms/sample - loss: 0.0454 - acc: 0.9899 - val_loss: 2.5933 - val_acc: 0.7502\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 2.5933 - acc: 0.7502\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 133s 11ms/sample - loss: 0.0481 - acc: 0.9900 - val_loss: 2.8744 - val_acc: 0.7164\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 2.8744 - acc: 0.7164\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.0995 - acc: 0.9811 - val_loss: 2.4145 - val_acc: 0.7678\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.4145 - acc: 0.7678\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.1393 - acc: 0.9652 - val_loss: 2.0933 - val_acc: 0.7628\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.0933 - acc: 0.7628\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 130s 11ms/sample - loss: 0.0275 - acc: 0.9938 - val_loss: 3.0650 - val_acc: 0.7516\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 3.0650 - acc: 0.7516\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.0587 - acc: 0.9852 - val_loss: 2.7354 - val_acc: 0.7120\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.7354 - acc: 0.7120\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.1163 - acc: 0.9757 - val_loss: 2.2143 - val_acc: 0.7286\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.2143 - acc: 0.7286\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.0283 - acc: 0.9922 - val_loss: 3.2647 - val_acc: 0.7720\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 3.2647 - acc: 0.7720\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.0203 - acc: 0.9942 - val_loss: 2.8556 - val_acc: 0.7674\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.8556 - acc: 0.7674\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.0024 - acc: 0.9991 - val_loss: 2.8492 - val_acc: 0.7920\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 2.8492 - acc: 0.7920\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 2.7730e-04 - acc: 0.9999 - val_loss: 2.9709 - val_acc: 0.7928\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.9709 - acc: 0.7928\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 130s 11ms/sample - loss: 5.5004e-04 - acc: 0.9998 - val_loss: 3.3507 - val_acc: 0.7902\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 3.3507 - acc: 0.7902\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.0651 - acc: 0.9823 - val_loss: 2.6268 - val_acc: 0.7414\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.6268 - acc: 0.7414\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.0189 - acc: 0.9950 - val_loss: 2.2156 - val_acc: 0.7662\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 2.2156 - acc: 0.7662\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.0033 - acc: 0.9990 - val_loss: 2.9514 - val_acc: 0.7738\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.9514 - acc: 0.7738\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 130s 11ms/sample - loss: 0.0943 - acc: 0.9755 - val_loss: 2.1765 - val_acc: 0.7522\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.1765 - acc: 0.7522\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 0.0080 - acc: 0.9983 - val_loss: 2.6208 - val_acc: 0.7628\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.6208 - acc: 0.7628\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 130s 11ms/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 2.4816 - val_acc: 0.7654\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 2.4816 - acc: 0.7654\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 1.5069e-04 - acc: 1.0000 - val_loss: 2.6089 - val_acc: 0.7640\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 2.6089 - acc: 0.7640\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 4.7557e-05 - acc: 1.0000 - val_loss: 2.7122 - val_acc: 0.7664\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 2.7122 - acc: 0.7664\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 2.5245e-05 - acc: 1.0000 - val_loss: 2.7963 - val_acc: 0.7706\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 2.7963 - acc: 0.7706\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 1.5947e-05 - acc: 1.0000 - val_loss: 2.8598 - val_acc: 0.7722\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 2.8598 - acc: 0.7722\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 1.0923e-05 - acc: 1.0000 - val_loss: 2.9122 - val_acc: 0.7718\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 2.9122 - acc: 0.7718\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 7.7649e-06 - acc: 1.0000 - val_loss: 2.9615 - val_acc: 0.7720\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 2.9615 - acc: 0.7720\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 5.6763e-06 - acc: 1.0000 - val_loss: 3.0115 - val_acc: 0.7740\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 3.0115 - acc: 0.7740\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 4.2359e-06 - acc: 1.0000 - val_loss: 3.0559 - val_acc: 0.7744\n",
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 3.0559 - acc: 0.7744\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 3.2268e-06 - acc: 1.0000 - val_loss: 3.1003 - val_acc: 0.7748\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 3.1003 - acc: 0.7748\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 2.4880e-06 - acc: 1.0000 - val_loss: 3.1458 - val_acc: 0.7744\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 3.1458 - acc: 0.7744\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 1.9401e-06 - acc: 1.0000 - val_loss: 3.1872 - val_acc: 0.7750\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 3.1872 - acc: 0.7750\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 132s 11ms/sample - loss: 1.5211e-06 - acc: 1.0000 - val_loss: 3.2314 - val_acc: 0.7750\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 3.2314 - acc: 0.7750\n",
      "Train on 12000 samples, validate on 5000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 132s 11ms/sample - loss: 1.2011e-06 - acc: 1.0000 - val_loss: 3.2736 - val_acc: 0.7756\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 3.2736 - acc: 0.7756\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 132s 11ms/sample - loss: 9.5351e-07 - acc: 1.0000 - val_loss: 3.3153 - val_acc: 0.7760\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 3.3153 - acc: 0.7760\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      "12000/12000 [==============================] - 131s 11ms/sample - loss: 7.5743e-07 - acc: 1.0000 - val_loss: 3.3576 - val_acc: 0.7774\n",
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 3.3576 - acc: 0.7774\n",
      "Train on 12000 samples, validate on 5000 samples\n",
      " 6112/12000 [==============>...............] - ETA: 1:00 - loss: 6.6791e-07 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "modelo_adam = keras.optimizers.Adam(learning_rate=0.0008)\n",
    "modelo.compile(optimizer=modelo_adam,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "v_eA=[]\n",
    "v_aA=[]\n",
    "evaluacion(modelo,82,\"ADAM\",v_eA,v_aA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5dacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.save(\"D:/UMSA/Documentos/CNN cancer de piel/MODELOS/17000_2_2/modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f346ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.save_weights(\"D:/UMSA/Documentos/CNN cancer de piel/PESOS/17000_2_2/modelo_pesos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1\n",
    "print(\"# de EPOCAS     Valor del Accuracy\")\n",
    "print(\"----------------------------------\")\n",
    "for i in v_aA:\n",
    "    print(\"epoca \",count,\" => \",i)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fea0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(v_aA,v_eA, marker='o', linestyle=':', color='b', label = \"ADAM\")\n",
    "\n",
    "#mp.xticks(np.arange(70,100,2))\n",
    "#mp.yticks(np.arange(0,4,1))\n",
    "mp.xlabel(\"porcentaje de asertividad\")\n",
    "mp.ylabel(\"Numero de epocas\")\n",
    "mp.legend(loc=\"upper left\")\n",
    "mp.title(\"Nivel de asertividad y epocas con 10000 imagenes\")\n",
    "mp.grid(True)\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e2f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cargado=keras.models.load_model(\"D:/UMSA/Documentos/CNN cancer de piel/MODELOS/12500/modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aec762",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cargado.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a94bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(v_aA,v_eA, marker='o', linestyle=':', color='b', label = \"ADAM\")\n",
    "\n",
    "#mp.xticks(np.arange(70,100,2))\n",
    "#mp.yticks(np.arange(0,4,1))\n",
    "mp.xlabel(\"porcentaje de asertividad\")\n",
    "mp.ylabel(\"Numero de epocas\")\n",
    "mp.legend(loc=\"upper left\")\n",
    "mp.title(\"Nivel de asertividad y epocas con 17000 imagenes\")\n",
    "mp.grid(True)\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22268183",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_probs = modelo.predict_proba(x_p)\n",
    "lr_probs = lr_probs[:, 0]\n",
    "ns_probs = [0 for _ in range(len(y_p))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b207ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a674a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el AUC\n",
    "ns_auc = roc_auc_score(y_p, ns_probs)\n",
    "lr_auc = roc_auc_score(y_p, lr_probs)\n",
    "# Imprimimos en pantalla\n",
    "print('Sin entrenar: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Red Neuronal: ROC AUC=%.3f' % (lr_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos las curvas ROC\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_p, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_p, lr_probs)\n",
    "# Pintamos las curvas ROC\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='Sin entrenar')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Red Neuronal')\n",
    "# Etiquetas de los ejes\n",
    "pyplot.xlabel('Tasa de Falsos Positivos')\n",
    "pyplot.ylabel('Tasa de Verdaderos Positivos')\n",
    "pyplot.legend()\n",
    "mp.grid(True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_probs = modelo.predict_proba(x_p)\n",
    "# Nos quedamos unicamente con las predicciones positicas\n",
    "lr_probs = lr_probs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce60d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = modelo.predict(x_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de31c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_precision, lr_recall, _ = precision_recall_curve(y_p, lr_probs)\n",
    "lr_auc =  auc(lr_recall, lr_precision)\n",
    "no_skill = len(y_p[y_p==1]) / len(y_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feaeb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Redes neuronales: auc=%.3f' % (lr_auc))\n",
    "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='Sin entrenar')\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Redes Neuronales')\n",
    "#Etiquetas de ejes\n",
    "pyplot.xlabel('Sensibilidad')\n",
    "pyplot.ylabel('Precisi√≥n')\n",
    "pyplot.legend()\n",
    "mp.grid(True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f0d51",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
